{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Contrastive_model_training.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKvxmjl84oxr",
        "colab_type": "code",
        "outputId": "1f6e6a92-6302-410b-f968-53fd52efa5f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "pip uninstall tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6sE1-74xKs",
        "colab_type": "code",
        "outputId": "6e5a6214-453a-44ec-b798-260f3a621f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "pip install tensorflow==1.13.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 58kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.29.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.9.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (47.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh3wAg5q5Yby",
        "colab_type": "code",
        "outputId": "1a2c3119-b96e-40a8-db7d-a9828c83f13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (47.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3014858 sha256=442c048c8bc11118ea0181316bb37ef846aa71947842f2168e9dd666c16c0868\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLdWtYorI4hI",
        "colab_type": "code",
        "outputId": "2376a2ce-ed95-4873-9e3d-ade04a3df216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.contrib import learn\n",
        "import fasttext\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q1IoMGkMX78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreProcessing:\n",
        "    def __init__(self,data_src):\n",
        "\n",
        "        self.similar_pairs = self.build_corpus('Contrastive_loss_dataset.csv')\n",
        "        self.embeddings_model = fasttext.train_unsupervised(\"text_corpus.txt\", model='skipgram', lr=0.005, dim=64,\n",
        "                                            ws=5, epoch=200)\n",
        "        self.embeddings_model.save_model(\"ft_skipgram_ws5_dim64.bin\")\n",
        "        print('FastText training finished successfully.')\n",
        "        self.current_index = 0\n",
        "        input_X = list(self.similar_pairs['Possible Questions'])\n",
        "        input_Y = list(self.similar_pairs['Atom'])\n",
        "        wc_list_x = list(len(x.split(' ')) for x in input_X)\n",
        "        wc_list_y = list(len(x.split(' ')) for x in input_Y)\n",
        "        wc_list = []\n",
        "        wc_list.extend(wc_list_x)\n",
        "        wc_list.extend(wc_list_y)\n",
        "        max_document_length = 16                 # or use a constant like 16, select this parameter based on your understanding of what could be a good choice\n",
        "        number_of_elements = len(input_X)\n",
        "        self.vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "        full_corpus = []\n",
        "        full_corpus.extend(input_X)\n",
        "        full_corpus.extend(input_Y)\n",
        "        full_data = np.asarray(list(self.vocab_processor.fit_transform(full_corpus)))\n",
        "        self.embeddings_lookup = []\n",
        "        for word in list(self.vocab_processor.vocabulary_._mapping):\n",
        "            try:\n",
        "                self.embeddings_lookup.append(self.embeddings_model[str(word)])\n",
        "            except:\n",
        "                pass\n",
        "        self.embeddings_lookup = np.asarray(self.embeddings_lookup)\n",
        "        self.vocab_processor.save('./model_siamese_network/vocab')\n",
        "        self.write_metadata(os.path.join('model_siamese_network','metadata.tsv'),list(self.vocab_processor.vocabulary_._mapping))\n",
        "        print('Vocab processor executed and saved successfully.')\n",
        "        self.X = full_data[0:number_of_elements]\n",
        "        self.Y = full_data[number_of_elements:2*number_of_elements]\n",
        "        self.label = list(self.similar_pairs['Relevant_question'])\n",
        "\n",
        "    def preprocess(self,x):\n",
        "        try:\n",
        "            tk_x = x.lower()\n",
        "\n",
        "            # list of characters which needs to be replaced with space\n",
        "            space_replace_chars = ['?', ':', ',', '\"', '[', ']', '~', '*', ';', '!', '?', '(', ')', '{', '}', '@', '$',\n",
        "                                   '#', '.', '-', '/']\n",
        "            tk_x = tk_x.translate({ord(x): ' ' for x in space_replace_chars})\n",
        "\n",
        "            non_space_replace_chars = [\"'\"]\n",
        "            tk_x = tk_x.translate({ord(x): '' for x in non_space_replace_chars})\n",
        "\n",
        "            # remove non-ASCII chars\n",
        "            tk_x = ''.join([c if ord(c) < 128 else '' for c in tk_x])\n",
        "\n",
        "            # replace all consecutive spaces with one space\n",
        "            tk_x = re.sub('\\s+', ' ', tk_x).strip()\n",
        "\n",
        "            # find all consecutive numbers present in the word, first converted numbers to * to prevent conflicts while replacing with numbers\n",
        "            regex = re.compile(r'([\\d])')\n",
        "            tk_x = regex.sub('*', tk_x)\n",
        "            nos = re.findall(r'([\\*]+)', tk_x)\n",
        "            # replace the numbers with the corresponding count like 123 by 3\n",
        "            for no in nos:\n",
        "                tk_x = tk_x.replace(no, \"<NUMBER>\", 1)\n",
        "\n",
        "            return tk_x.strip().lower()\n",
        "        except:\n",
        "            return \"\"\n",
        "\n",
        "    def build_corpus(self, filepath):\n",
        "        similar_items = pd.read_csv(filepath)\n",
        "        selected_cols = ['Possible Questions', 'Atom', 'Relevant_question']\n",
        "        similar_items = similar_items[selected_cols]\n",
        "        similar_items['Possible Questions'] = similar_items['Possible Questions'].apply(self.preprocess)\n",
        "        similar_items['Atom'] = similar_items['Atom'].apply(self.preprocess)\n",
        "        similar_items = shuffle(similar_items)\n",
        "        similar_items = similar_items.drop_duplicates()\n",
        "        question_list = list(similar_items['Possible Questions'])\n",
        "        question_list.extend(list(similar_items['Atom']))\n",
        "        pd.DataFrame(question_list).to_csv('text_corpus.txt', index=False)\n",
        "        print('Text corpus generated and persisted successfully.')\n",
        "        return similar_items\n",
        "\n",
        "    def write_metadata(self,filename, labels):\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(\"Index\\tLabel\\n\")\n",
        "            for index, label in enumerate(labels):\n",
        "                f.write(\"{}\\t{}\\n\".format(index, label))\n",
        "\n",
        "        print('Metadata file saved in {}'.format(filename))\n",
        "\n",
        "    def get_siamese_batch(self, n):\n",
        "        last_index = self.current_index\n",
        "        self.current_index += n\n",
        "        return self.X[last_index: self.current_index, :], self.Y[last_index: self.current_index, :], np.expand_dims(self.label[last_index: self.current_index], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgEAM5_XQdRf",
        "colab_type": "code",
        "outputId": "d0f8359d-d0bf-4c7e-f2b6-fe13e1541598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "dataset = PreProcessing('Contrastive_loss_dataset.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text corpus generated and persisted successfully.\n",
            "FastText training finished successfully.\n",
            "WARNING:tensorflow:From <ipython-input-2-7a138e1c88c8>:19: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "Metadata file saved in model_siamese_network/metadata.tsv\n",
            "Vocab processor executed and saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZLS5Oz5I4hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork:\n",
        "\n",
        "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters, output_embedding_size, dropout_keep_prob,embeddings_lookup, l2_reg_lambda=0.0):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.num_filters = num_filters\n",
        "        self.output_embedding_size = output_embedding_size\n",
        "        self.dropout_keep_prob = dropout_keep_prob\n",
        "        self.l2_loss = tf.constant(0.0)\n",
        "        self.embeddings_lookup = embeddings_lookup\n",
        "\n",
        "    def siamese_net(self, input_x, reuse=False):\n",
        "        \n",
        "        # Embedding layer\n",
        "        with tf.variable_scope(\"embedding\", reuse=reuse):\n",
        "            self.W = tf.Variable(self.embeddings_lookup, name=\"W\")\n",
        "            self.word_embeddings = tf.nn.embedding_lookup(self.W, input_x)\n",
        "            self.expanded_word_embeddings = tf.expand_dims(self.word_embeddings, -1)\n",
        "\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(self.filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.expanded_word_embeddings,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total], name=\"pool_flat\")\n",
        "\n",
        "        # Add dropout [comment dropout during prediction]\n",
        "        # with tf.name_scope(\"dropout\"):\n",
        "        #     h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob, name=\"dropout\")\n",
        "\n",
        "        # Final output Layer\n",
        "        with tf.variable_scope(\"output\", reuse=reuse):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, self.output_embedding_size],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b2 = tf.Variable(tf.constant(0.1, shape=[self.output_embedding_size]), name=\"b2\")\n",
        "            out_net = tf.nn.xw_plus_b(self.h_pool_flat, W, b2, name=\"output_embedding\")\n",
        "\n",
        "        return out_net\n",
        "\n",
        "    def contrastive_loss(self, model1, model2, y, margin):\n",
        "        with tf.name_scope(\"contrastive-loss\"):\n",
        "            distance = tf.sqrt(tf.reduce_sum(tf.pow(model1 - model2, 2), 1, keepdims=True))\n",
        "            return tf.reduce_mean(y * tf.square(distance) + (1 - y) * tf.square(tf.maximum((margin - distance), 0))) / 2 + 1e-9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOceSijdI4hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Hyper-parameters\n",
        "embedding_dim = 64\n",
        "filter_sizes = \"3,4,5\"\n",
        "num_filters = 64\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.0\n",
        "output_embedding_size = 64\n",
        "batch_size = 32\n",
        "train_iter = 500\n",
        "step = 50\n",
        "learning_rate = 0.0001\n",
        "momentum = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKRZaTUtI4hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SiameseNetwork(sequence_length=dataset.X.shape[1],\n",
        "            vocab_size=len(dataset.vocab_processor.vocabulary_),\n",
        "            embedding_size=embedding_dim,\n",
        "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
        "            num_filters=num_filters,\n",
        "            output_embedding_size = output_embedding_size,\n",
        "            dropout_keep_prob = dropout_keep_prob,\n",
        "            embeddings_lookup= dataset.embeddings_lookup,\n",
        "            l2_reg_lambda=l2_reg_lambda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWTdzEd7I4hb",
        "colab_type": "code",
        "outputId": "f8f3306f-df96-4909-f634-6bfdd55f6b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "placeholder_shape = [None] + [dataset.X.shape[1]]\n",
        "print(\"placeholder_shape\", placeholder_shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "placeholder_shape [None, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l20l387I4he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup Network\n",
        "next_batch = dataset.get_siamese_batch\n",
        "left_input = tf.placeholder(tf.int32, placeholder_shape, name='left_input')\n",
        "right_input = tf.placeholder(tf.int32, placeholder_shape, name='right_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQlKGVOKI4hj",
        "colab_type": "code",
        "outputId": "216f46ee-8c14-470f-a51d-15105f8b99eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "margin = 2.5\n",
        "left_output = model.siamese_net(left_input, reuse=False)\n",
        "right_output = model.siamese_net(right_input, reuse=True)\n",
        "with tf.name_scope(\"similarity\"):\n",
        "    label = tf.placeholder(tf.int32, [None, 1], name='label')\n",
        "    label_float = tf.to_float(label)\n",
        "loss = model.contrastive_loss(left_output, right_output, label_float, margin)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-9-6191bd138d81>:6: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGVQhPsEI4hn",
        "colab_type": "code",
        "outputId": "ba47436e-e438-4fd4-bad0-7de796125041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Setup Optimizer\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "train_step = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True).minimize(loss,global_step=global_step)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flINMF_qI4hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib.tensorboard.plugins import projector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_VoTgOI4hv",
        "colab_type": "code",
        "outputId": "8c967cfd-80bd-450f-aae9-9685b220fb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start Training\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Setup Tensorboard\n",
        "    tf.summary.scalar('step', global_step)\n",
        "    tf.summary.scalar('loss', loss)\n",
        "    for var in tf.trainable_variables():\n",
        "        tf.summary.histogram(var.op.name, var)\n",
        "    merged = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter('model_siamese_network', sess.graph)\n",
        "    # adding embeddings to projector\n",
        "    config = projector.ProjectorConfig()\n",
        "    embed = config.embeddings.add()\n",
        "    embed.tensor_name = \"embedding/W\"\n",
        "    embed.metadata_path = \"metadata.tsv\"\n",
        "    projector.visualize_embeddings(writer, config)\n",
        "    print('Training...')\n",
        "    # Batch Training\n",
        "    for i in range(train_iter):\n",
        "        batch_left, batch_right, batch_similarity_score = next_batch(batch_size)\n",
        "        _, l, summary_str = sess.run([train_step, loss, merged],\n",
        "                                     feed_dict={left_input: batch_left, right_input: batch_right, label: batch_similarity_score})\n",
        "        writer.add_summary(summary_str, i)\n",
        "        print(\"\\r#%d - Loss\" % i, l)\n",
        "        \n",
        "        if (i + 1) % step == 0 or l <= 1e-9:\n",
        "            print('Saving model. Step: ',i)\n",
        "            saver.save(sess, \"model_siamese_network/model.ckpt\")\n",
        "        if l=='nan':\n",
        "            break\n",
        "    saver.save(sess, \"model_siamese_network/model.ckpt\")\n",
        "print('Training completed successfully.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "\r#0 - Loss 1e-09\n",
            "Saving model. Step:  0\n",
            "#1 - Loss 1e-09\n",
            "Saving model. Step:  1\n",
            "#2 - Loss 1e-09\n",
            "Saving model. Step:  2\n",
            "#3 - Loss 1e-09\n",
            "Saving model. Step:  3\n",
            "#4 - Loss 1e-09\n",
            "Saving model. Step:  4\n",
            "#5 - Loss 1e-09\n",
            "Saving model. Step:  5\n",
            "#6 - Loss 1e-09\n",
            "Saving model. Step:  6\n",
            "#7 - Loss 1e-09\n",
            "Saving model. Step:  7\n",
            "#8 - Loss 1e-09\n",
            "Saving model. Step:  8\n",
            "#9 - Loss 1e-09\n",
            "Saving model. Step:  9\n",
            "#10 - Loss 1e-09\n",
            "Saving model. Step:  10\n",
            "#11 - Loss 1e-09\n",
            "Saving model. Step:  11\n",
            "#12 - Loss 1e-09\n",
            "Saving model. Step:  12\n",
            "#13 - Loss 1e-09\n",
            "Saving model. Step:  13\n",
            "#14 - Loss 1e-09\n",
            "Saving model. Step:  14\n",
            "#15 - Loss 1e-09\n",
            "Saving model. Step:  15\n",
            "#16 - Loss 1e-09\n",
            "Saving model. Step:  16\n",
            "#17 - Loss 1e-09\n",
            "Saving model. Step:  17\n",
            "#18 - Loss 1e-09\n",
            "Saving model. Step:  18\n",
            "#19 - Loss 1e-09\n",
            "Saving model. Step:  19\n",
            "#20 - Loss 1e-09\n",
            "Saving model. Step:  20\n",
            "#21 - Loss 1e-09\n",
            "Saving model. Step:  21\n",
            "#22 - Loss 1e-09\n",
            "Saving model. Step:  22\n",
            "#23 - Loss 1e-09\n",
            "Saving model. Step:  23\n",
            "#24 - Loss 1e-09\n",
            "Saving model. Step:  24\n",
            "#25 - Loss 1e-09\n",
            "Saving model. Step:  25\n",
            "#26 - Loss 1e-09\n",
            "Saving model. Step:  26\n",
            "#27 - Loss 1e-09\n",
            "Saving model. Step:  27\n",
            "#28 - Loss 1e-09\n",
            "Saving model. Step:  28\n",
            "#29 - Loss 1e-09\n",
            "Saving model. Step:  29\n",
            "#30 - Loss 1e-09\n",
            "Saving model. Step:  30\n",
            "#31 - Loss 1e-09\n",
            "Saving model. Step:  31\n",
            "#32 - Loss 1e-09\n",
            "Saving model. Step:  32\n",
            "#33 - Loss 1e-09\n",
            "Saving model. Step:  33\n",
            "#34 - Loss 1e-09\n",
            "Saving model. Step:  34\n",
            "#35 - Loss 1e-09\n",
            "Saving model. Step:  35\n",
            "#36 - Loss 1e-09\n",
            "Saving model. Step:  36\n",
            "#37 - Loss 1e-09\n",
            "Saving model. Step:  37\n",
            "#38 - Loss 1e-09\n",
            "Saving model. Step:  38\n",
            "#39 - Loss 1e-09\n",
            "Saving model. Step:  39\n",
            "#40 - Loss 1e-09\n",
            "Saving model. Step:  40\n",
            "#41 - Loss 1e-09\n",
            "Saving model. Step:  41\n",
            "#42 - Loss 1e-09\n",
            "Saving model. Step:  42\n",
            "#43 - Loss 1e-09\n",
            "Saving model. Step:  43\n",
            "#44 - Loss 1e-09\n",
            "Saving model. Step:  44\n",
            "#45 - Loss 1e-09\n",
            "Saving model. Step:  45\n",
            "#46 - Loss 1e-09\n",
            "Saving model. Step:  46\n",
            "#47 - Loss 1e-09\n",
            "Saving model. Step:  47\n",
            "#48 - Loss 1e-09\n",
            "Saving model. Step:  48\n",
            "#49 - Loss 1e-09\n",
            "Saving model. Step:  49\n",
            "#50 - Loss 1e-09\n",
            "Saving model. Step:  50\n",
            "#51 - Loss 1e-09\n",
            "Saving model. Step:  51\n",
            "#52 - Loss 1e-09\n",
            "Saving model. Step:  52\n",
            "#53 - Loss 1e-09\n",
            "Saving model. Step:  53\n",
            "#54 - Loss 1e-09\n",
            "Saving model. Step:  54\n",
            "#55 - Loss 1e-09\n",
            "Saving model. Step:  55\n",
            "#56 - Loss 1e-09\n",
            "Saving model. Step:  56\n",
            "#57 - Loss 1e-09\n",
            "Saving model. Step:  57\n",
            "#58 - Loss 1e-09\n",
            "Saving model. Step:  58\n",
            "#59 - Loss 1e-09\n",
            "Saving model. Step:  59\n",
            "#60 - Loss 1e-09\n",
            "Saving model. Step:  60\n",
            "#61 - Loss 1e-09\n",
            "Saving model. Step:  61\n",
            "#62 - Loss 1e-09\n",
            "Saving model. Step:  62\n",
            "#63 - Loss 1e-09\n",
            "Saving model. Step:  63\n",
            "#64 - Loss 1e-09\n",
            "Saving model. Step:  64\n",
            "#65 - Loss 1e-09\n",
            "Saving model. Step:  65\n",
            "#66 - Loss 1e-09\n",
            "Saving model. Step:  66\n",
            "#67 - Loss 1e-09\n",
            "Saving model. Step:  67\n",
            "#68 - Loss 1e-09\n",
            "Saving model. Step:  68\n",
            "#69 - Loss 1e-09\n",
            "Saving model. Step:  69\n",
            "#70 - Loss 1e-09\n",
            "Saving model. Step:  70\n",
            "#71 - Loss 1e-09\n",
            "Saving model. Step:  71\n",
            "#72 - Loss 1e-09\n",
            "Saving model. Step:  72\n",
            "#73 - Loss 1e-09\n",
            "Saving model. Step:  73\n",
            "#74 - Loss 1e-09\n",
            "Saving model. Step:  74\n",
            "#75 - Loss 1e-09\n",
            "Saving model. Step:  75\n",
            "#76 - Loss 1e-09\n",
            "Saving model. Step:  76\n",
            "#77 - Loss 1e-09\n",
            "Saving model. Step:  77\n",
            "#78 - Loss 1e-09\n",
            "Saving model. Step:  78\n",
            "#79 - Loss 1e-09\n",
            "Saving model. Step:  79\n",
            "#80 - Loss 1e-09\n",
            "Saving model. Step:  80\n",
            "#81 - Loss 1e-09\n",
            "Saving model. Step:  81\n",
            "#82 - Loss 1e-09\n",
            "Saving model. Step:  82\n",
            "#83 - Loss 1e-09\n",
            "Saving model. Step:  83\n",
            "#84 - Loss 1e-09\n",
            "Saving model. Step:  84\n",
            "#85 - Loss 1e-09\n",
            "Saving model. Step:  85\n",
            "#86 - Loss 1e-09\n",
            "Saving model. Step:  86\n",
            "#87 - Loss 1e-09\n",
            "Saving model. Step:  87\n",
            "#88 - Loss 1e-09\n",
            "Saving model. Step:  88\n",
            "#89 - Loss 1e-09\n",
            "Saving model. Step:  89\n",
            "#90 - Loss 1e-09\n",
            "Saving model. Step:  90\n",
            "#91 - Loss 1e-09\n",
            "Saving model. Step:  91\n",
            "#92 - Loss 1e-09\n",
            "Saving model. Step:  92\n",
            "#93 - Loss 1e-09\n",
            "Saving model. Step:  93\n",
            "#94 - Loss 1e-09\n",
            "Saving model. Step:  94\n",
            "#95 - Loss 1e-09\n",
            "Saving model. Step:  95\n",
            "#96 - Loss 1e-09\n",
            "Saving model. Step:  96\n",
            "#97 - Loss 1e-09\n",
            "Saving model. Step:  97\n",
            "#98 - Loss 1e-09\n",
            "Saving model. Step:  98\n",
            "#99 - Loss 1e-09\n",
            "Saving model. Step:  99\n",
            "#100 - Loss 1e-09\n",
            "Saving model. Step:  100\n",
            "#101 - Loss 1e-09\n",
            "Saving model. Step:  101\n",
            "#102 - Loss 1e-09\n",
            "Saving model. Step:  102\n",
            "#103 - Loss 1e-09\n",
            "Saving model. Step:  103\n",
            "#104 - Loss 1e-09\n",
            "Saving model. Step:  104\n",
            "#105 - Loss 1e-09\n",
            "Saving model. Step:  105\n",
            "#106 - Loss 1e-09\n",
            "Saving model. Step:  106\n",
            "#107 - Loss 1e-09\n",
            "Saving model. Step:  107\n",
            "#108 - Loss 1e-09\n",
            "Saving model. Step:  108\n",
            "#109 - Loss 1e-09\n",
            "Saving model. Step:  109\n",
            "#110 - Loss 1e-09\n",
            "Saving model. Step:  110\n",
            "#111 - Loss 1e-09\n",
            "Saving model. Step:  111\n",
            "#112 - Loss 1e-09\n",
            "Saving model. Step:  112\n",
            "#113 - Loss 1e-09\n",
            "Saving model. Step:  113\n",
            "#114 - Loss 1e-09\n",
            "Saving model. Step:  114\n",
            "#115 - Loss 1e-09\n",
            "Saving model. Step:  115\n",
            "#116 - Loss 1e-09\n",
            "Saving model. Step:  116\n",
            "#117 - Loss 1e-09\n",
            "Saving model. Step:  117\n",
            "#118 - Loss 1e-09\n",
            "Saving model. Step:  118\n",
            "#119 - Loss 1e-09\n",
            "Saving model. Step:  119\n",
            "#120 - Loss 1e-09\n",
            "Saving model. Step:  120\n",
            "#121 - Loss 1e-09\n",
            "Saving model. Step:  121\n",
            "#122 - Loss 1e-09\n",
            "Saving model. Step:  122\n",
            "#123 - Loss 1e-09\n",
            "Saving model. Step:  123\n",
            "#124 - Loss 1e-09\n",
            "Saving model. Step:  124\n",
            "#125 - Loss 1e-09\n",
            "Saving model. Step:  125\n",
            "#126 - Loss 1e-09\n",
            "Saving model. Step:  126\n",
            "#127 - Loss 1e-09\n",
            "Saving model. Step:  127\n",
            "#128 - Loss 1e-09\n",
            "Saving model. Step:  128\n",
            "#129 - Loss 1e-09\n",
            "Saving model. Step:  129\n",
            "#130 - Loss 1e-09\n",
            "Saving model. Step:  130\n",
            "#131 - Loss 1e-09\n",
            "Saving model. Step:  131\n",
            "#132 - Loss 1e-09\n",
            "Saving model. Step:  132\n",
            "#133 - Loss 1e-09\n",
            "Saving model. Step:  133\n",
            "#134 - Loss 1e-09\n",
            "Saving model. Step:  134\n",
            "#135 - Loss 1e-09\n",
            "Saving model. Step:  135\n",
            "#136 - Loss 1e-09\n",
            "Saving model. Step:  136\n",
            "#137 - Loss 1e-09\n",
            "Saving model. Step:  137\n",
            "#138 - Loss 1e-09\n",
            "Saving model. Step:  138\n",
            "#139 - Loss 1e-09\n",
            "Saving model. Step:  139\n",
            "#140 - Loss 1e-09\n",
            "Saving model. Step:  140\n",
            "#141 - Loss 1e-09\n",
            "Saving model. Step:  141\n",
            "#142 - Loss 1e-09\n",
            "Saving model. Step:  142\n",
            "#143 - Loss 1e-09\n",
            "Saving model. Step:  143\n",
            "#144 - Loss 1e-09\n",
            "Saving model. Step:  144\n",
            "#145 - Loss 1e-09\n",
            "Saving model. Step:  145\n",
            "#146 - Loss 1e-09\n",
            "Saving model. Step:  146\n",
            "#147 - Loss 1e-09\n",
            "Saving model. Step:  147\n",
            "#148 - Loss 1e-09\n",
            "Saving model. Step:  148\n",
            "#149 - Loss 1e-09\n",
            "Saving model. Step:  149\n",
            "#150 - Loss 1e-09\n",
            "Saving model. Step:  150\n",
            "#151 - Loss 1e-09\n",
            "Saving model. Step:  151\n",
            "#152 - Loss 1e-09\n",
            "Saving model. Step:  152\n",
            "#153 - Loss 1e-09\n",
            "Saving model. Step:  153\n",
            "#154 - Loss 1e-09\n",
            "Saving model. Step:  154\n",
            "#155 - Loss 1e-09\n",
            "Saving model. Step:  155\n",
            "#156 - Loss 1e-09\n",
            "Saving model. Step:  156\n",
            "#157 - Loss 1e-09\n",
            "Saving model. Step:  157\n",
            "#158 - Loss 1e-09\n",
            "Saving model. Step:  158\n",
            "#159 - Loss 1e-09\n",
            "Saving model. Step:  159\n",
            "#160 - Loss 1e-09\n",
            "Saving model. Step:  160\n",
            "#161 - Loss 1e-09\n",
            "Saving model. Step:  161\n",
            "#162 - Loss 1e-09\n",
            "Saving model. Step:  162\n",
            "#163 - Loss 1e-09\n",
            "Saving model. Step:  163\n",
            "#164 - Loss 1e-09\n",
            "Saving model. Step:  164\n",
            "#165 - Loss 1e-09\n",
            "Saving model. Step:  165\n",
            "#166 - Loss 1e-09\n",
            "Saving model. Step:  166\n",
            "#167 - Loss 1e-09\n",
            "Saving model. Step:  167\n",
            "#168 - Loss 1e-09\n",
            "Saving model. Step:  168\n",
            "#169 - Loss 1e-09\n",
            "Saving model. Step:  169\n",
            "#170 - Loss 1e-09\n",
            "Saving model. Step:  170\n",
            "#171 - Loss 1e-09\n",
            "Saving model. Step:  171\n",
            "#172 - Loss 1e-09\n",
            "Saving model. Step:  172\n",
            "#173 - Loss 1e-09\n",
            "Saving model. Step:  173\n",
            "#174 - Loss 1e-09\n",
            "Saving model. Step:  174\n",
            "#175 - Loss 1e-09\n",
            "Saving model. Step:  175\n",
            "#176 - Loss 1e-09\n",
            "Saving model. Step:  176\n",
            "#177 - Loss 1e-09\n",
            "Saving model. Step:  177\n",
            "#178 - Loss 1e-09\n",
            "Saving model. Step:  178\n",
            "#179 - Loss 1e-09\n",
            "Saving model. Step:  179\n",
            "#180 - Loss 1e-09\n",
            "Saving model. Step:  180\n",
            "#181 - Loss 1e-09\n",
            "Saving model. Step:  181\n",
            "#182 - Loss 1e-09\n",
            "Saving model. Step:  182\n",
            "#183 - Loss 1e-09\n",
            "Saving model. Step:  183\n",
            "#184 - Loss 1e-09\n",
            "Saving model. Step:  184\n",
            "#185 - Loss 1e-09\n",
            "Saving model. Step:  185\n",
            "#186 - Loss 1e-09\n",
            "Saving model. Step:  186\n",
            "#187 - Loss 1e-09\n",
            "Saving model. Step:  187\n",
            "#188 - Loss 1e-09\n",
            "Saving model. Step:  188\n",
            "#189 - Loss 1e-09\n",
            "Saving model. Step:  189\n",
            "#190 - Loss 1e-09\n",
            "Saving model. Step:  190\n",
            "#191 - Loss 1e-09\n",
            "Saving model. Step:  191\n",
            "#192 - Loss 1e-09\n",
            "Saving model. Step:  192\n",
            "#193 - Loss 1e-09\n",
            "Saving model. Step:  193\n",
            "#194 - Loss 1e-09\n",
            "Saving model. Step:  194\n",
            "#195 - Loss 1e-09\n",
            "Saving model. Step:  195\n",
            "#196 - Loss 1e-09\n",
            "Saving model. Step:  196\n",
            "#197 - Loss 1e-09\n",
            "Saving model. Step:  197\n",
            "#198 - Loss 1e-09\n",
            "Saving model. Step:  198\n",
            "#199 - Loss 1e-09\n",
            "Saving model. Step:  199\n",
            "#200 - Loss 1e-09\n",
            "Saving model. Step:  200\n",
            "#201 - Loss 1e-09\n",
            "Saving model. Step:  201\n",
            "#202 - Loss 1e-09\n",
            "Saving model. Step:  202\n",
            "#203 - Loss 1e-09\n",
            "Saving model. Step:  203\n",
            "#204 - Loss 1e-09\n",
            "Saving model. Step:  204\n",
            "#205 - Loss 1e-09\n",
            "Saving model. Step:  205\n",
            "#206 - Loss 1e-09\n",
            "Saving model. Step:  206\n",
            "#207 - Loss 1e-09\n",
            "Saving model. Step:  207\n",
            "#208 - Loss 1e-09\n",
            "Saving model. Step:  208\n",
            "#209 - Loss 1e-09\n",
            "Saving model. Step:  209\n",
            "#210 - Loss 1e-09\n",
            "Saving model. Step:  210\n",
            "#211 - Loss 1e-09\n",
            "Saving model. Step:  211\n",
            "#212 - Loss 1e-09\n",
            "Saving model. Step:  212\n",
            "#213 - Loss 1e-09\n",
            "Saving model. Step:  213\n",
            "#214 - Loss 1e-09\n",
            "Saving model. Step:  214\n",
            "#215 - Loss 1e-09\n",
            "Saving model. Step:  215\n",
            "#216 - Loss 1e-09\n",
            "Saving model. Step:  216\n",
            "#217 - Loss 1e-09\n",
            "Saving model. Step:  217\n",
            "#218 - Loss 1e-09\n",
            "Saving model. Step:  218\n",
            "#219 - Loss 1e-09\n",
            "Saving model. Step:  219\n",
            "#220 - Loss nan\n",
            "#221 - Loss nan\n",
            "#222 - Loss nan\n",
            "#223 - Loss nan\n",
            "#224 - Loss nan\n",
            "#225 - Loss nan\n",
            "#226 - Loss nan\n",
            "#227 - Loss nan\n",
            "#228 - Loss nan\n",
            "#229 - Loss nan\n",
            "#230 - Loss nan\n",
            "#231 - Loss nan\n",
            "#232 - Loss nan\n",
            "#233 - Loss nan\n",
            "#234 - Loss nan\n",
            "#235 - Loss nan\n",
            "#236 - Loss nan\n",
            "#237 - Loss nan\n",
            "#238 - Loss nan\n",
            "#239 - Loss nan\n",
            "#240 - Loss nan\n",
            "#241 - Loss nan\n",
            "#242 - Loss nan\n",
            "#243 - Loss nan\n",
            "#244 - Loss nan\n",
            "#245 - Loss nan\n",
            "#246 - Loss nan\n",
            "#247 - Loss nan\n",
            "#248 - Loss nan\n",
            "#249 - Loss nan\n",
            "Saving model. Step:  249\n",
            "#250 - Loss nan\n",
            "#251 - Loss nan\n",
            "#252 - Loss nan\n",
            "#253 - Loss nan\n",
            "#254 - Loss nan\n",
            "#255 - Loss nan\n",
            "#256 - Loss nan\n",
            "#257 - Loss nan\n",
            "#258 - Loss nan\n",
            "#259 - Loss nan\n",
            "#260 - Loss nan\n",
            "#261 - Loss nan\n",
            "#262 - Loss nan\n",
            "#263 - Loss nan\n",
            "#264 - Loss nan\n",
            "#265 - Loss nan\n",
            "#266 - Loss nan\n",
            "#267 - Loss nan\n",
            "#268 - Loss nan\n",
            "#269 - Loss nan\n",
            "#270 - Loss nan\n",
            "#271 - Loss nan\n",
            "#272 - Loss nan\n",
            "#273 - Loss nan\n",
            "#274 - Loss nan\n",
            "#275 - Loss nan\n",
            "#276 - Loss nan\n",
            "#277 - Loss nan\n",
            "#278 - Loss nan\n",
            "#279 - Loss nan\n",
            "#280 - Loss nan\n",
            "#281 - Loss nan\n",
            "#282 - Loss nan\n",
            "#283 - Loss nan\n",
            "#284 - Loss nan\n",
            "#285 - Loss nan\n",
            "#286 - Loss nan\n",
            "#287 - Loss nan\n",
            "#288 - Loss nan\n",
            "#289 - Loss nan\n",
            "#290 - Loss nan\n",
            "#291 - Loss nan\n",
            "#292 - Loss nan\n",
            "#293 - Loss nan\n",
            "#294 - Loss nan\n",
            "#295 - Loss nan\n",
            "#296 - Loss nan\n",
            "#297 - Loss nan\n",
            "#298 - Loss nan\n",
            "#299 - Loss nan\n",
            "Saving model. Step:  299\n",
            "#300 - Loss nan\n",
            "#301 - Loss nan\n",
            "#302 - Loss nan\n",
            "#303 - Loss nan\n",
            "#304 - Loss nan\n",
            "#305 - Loss nan\n",
            "#306 - Loss nan\n",
            "#307 - Loss nan\n",
            "#308 - Loss nan\n",
            "#309 - Loss nan\n",
            "#310 - Loss nan\n",
            "#311 - Loss nan\n",
            "#312 - Loss nan\n",
            "#313 - Loss nan\n",
            "#314 - Loss nan\n",
            "#315 - Loss nan\n",
            "#316 - Loss nan\n",
            "#317 - Loss nan\n",
            "#318 - Loss nan\n",
            "#319 - Loss nan\n",
            "#320 - Loss nan\n",
            "#321 - Loss nan\n",
            "#322 - Loss nan\n",
            "#323 - Loss nan\n",
            "#324 - Loss nan\n",
            "#325 - Loss nan\n",
            "#326 - Loss nan\n",
            "#327 - Loss nan\n",
            "#328 - Loss nan\n",
            "#329 - Loss nan\n",
            "#330 - Loss nan\n",
            "#331 - Loss nan\n",
            "#332 - Loss nan\n",
            "#333 - Loss nan\n",
            "#334 - Loss nan\n",
            "#335 - Loss nan\n",
            "#336 - Loss nan\n",
            "#337 - Loss nan\n",
            "#338 - Loss nan\n",
            "#339 - Loss nan\n",
            "#340 - Loss nan\n",
            "#341 - Loss nan\n",
            "#342 - Loss nan\n",
            "#343 - Loss nan\n",
            "#344 - Loss nan\n",
            "#345 - Loss nan\n",
            "#346 - Loss nan\n",
            "#347 - Loss nan\n",
            "#348 - Loss nan\n",
            "#349 - Loss nan\n",
            "Saving model. Step:  349\n",
            "#350 - Loss nan\n",
            "#351 - Loss nan\n",
            "#352 - Loss nan\n",
            "#353 - Loss nan\n",
            "#354 - Loss nan\n",
            "#355 - Loss nan\n",
            "#356 - Loss nan\n",
            "#357 - Loss nan\n",
            "#358 - Loss nan\n",
            "#359 - Loss nan\n",
            "#360 - Loss nan\n",
            "#361 - Loss nan\n",
            "#362 - Loss nan\n",
            "#363 - Loss nan\n",
            "#364 - Loss nan\n",
            "#365 - Loss nan\n",
            "#366 - Loss nan\n",
            "#367 - Loss nan\n",
            "#368 - Loss nan\n",
            "#369 - Loss nan\n",
            "#370 - Loss nan\n",
            "#371 - Loss nan\n",
            "#372 - Loss nan\n",
            "#373 - Loss nan\n",
            "#374 - Loss nan\n",
            "#375 - Loss nan\n",
            "#376 - Loss nan\n",
            "#377 - Loss nan\n",
            "#378 - Loss nan\n",
            "#379 - Loss nan\n",
            "#380 - Loss nan\n",
            "#381 - Loss nan\n",
            "#382 - Loss nan\n",
            "#383 - Loss nan\n",
            "#384 - Loss nan\n",
            "#385 - Loss nan\n",
            "#386 - Loss nan\n",
            "#387 - Loss nan\n",
            "#388 - Loss nan\n",
            "#389 - Loss nan\n",
            "#390 - Loss nan\n",
            "#391 - Loss nan\n",
            "#392 - Loss nan\n",
            "#393 - Loss nan\n",
            "#394 - Loss nan\n",
            "#395 - Loss nan\n",
            "#396 - Loss nan\n",
            "#397 - Loss nan\n",
            "#398 - Loss nan\n",
            "#399 - Loss nan\n",
            "Saving model. Step:  399\n",
            "#400 - Loss nan\n",
            "#401 - Loss nan\n",
            "#402 - Loss nan\n",
            "#403 - Loss nan\n",
            "#404 - Loss nan\n",
            "#405 - Loss nan\n",
            "#406 - Loss nan\n",
            "#407 - Loss nan\n",
            "#408 - Loss nan\n",
            "#409 - Loss nan\n",
            "#410 - Loss nan\n",
            "#411 - Loss nan\n",
            "#412 - Loss nan\n",
            "#413 - Loss nan\n",
            "#414 - Loss nan\n",
            "#415 - Loss nan\n",
            "#416 - Loss nan\n",
            "#417 - Loss nan\n",
            "#418 - Loss nan\n",
            "#419 - Loss nan\n",
            "#420 - Loss nan\n",
            "#421 - Loss nan\n",
            "#422 - Loss nan\n",
            "#423 - Loss nan\n",
            "#424 - Loss nan\n",
            "#425 - Loss nan\n",
            "#426 - Loss nan\n",
            "#427 - Loss nan\n",
            "#428 - Loss nan\n",
            "#429 - Loss nan\n",
            "#430 - Loss nan\n",
            "#431 - Loss nan\n",
            "#432 - Loss nan\n",
            "#433 - Loss nan\n",
            "#434 - Loss nan\n",
            "#435 - Loss nan\n",
            "#436 - Loss nan\n",
            "#437 - Loss nan\n",
            "#438 - Loss nan\n",
            "#439 - Loss nan\n",
            "#440 - Loss nan\n",
            "#441 - Loss nan\n",
            "#442 - Loss nan\n",
            "#443 - Loss nan\n",
            "#444 - Loss nan\n",
            "#445 - Loss nan\n",
            "#446 - Loss nan\n",
            "#447 - Loss nan\n",
            "#448 - Loss nan\n",
            "#449 - Loss nan\n",
            "Saving model. Step:  449\n",
            "#450 - Loss nan\n",
            "#451 - Loss nan\n",
            "#452 - Loss nan\n",
            "#453 - Loss nan\n",
            "#454 - Loss nan\n",
            "#455 - Loss nan\n",
            "#456 - Loss nan\n",
            "#457 - Loss nan\n",
            "#458 - Loss nan\n",
            "#459 - Loss nan\n",
            "#460 - Loss nan\n",
            "#461 - Loss nan\n",
            "#462 - Loss nan\n",
            "#463 - Loss nan\n",
            "#464 - Loss nan\n",
            "#465 - Loss nan\n",
            "#466 - Loss nan\n",
            "#467 - Loss nan\n",
            "#468 - Loss nan\n",
            "#469 - Loss nan\n",
            "#470 - Loss nan\n",
            "#471 - Loss nan\n",
            "#472 - Loss nan\n",
            "#473 - Loss nan\n",
            "#474 - Loss nan\n",
            "#475 - Loss nan\n",
            "#476 - Loss nan\n",
            "#477 - Loss nan\n",
            "#478 - Loss nan\n",
            "#479 - Loss nan\n",
            "#480 - Loss nan\n",
            "#481 - Loss nan\n",
            "#482 - Loss nan\n",
            "#483 - Loss nan\n",
            "#484 - Loss nan\n",
            "#485 - Loss nan\n",
            "#486 - Loss nan\n",
            "#487 - Loss nan\n",
            "#488 - Loss nan\n",
            "#489 - Loss nan\n",
            "#490 - Loss nan\n",
            "#491 - Loss nan\n",
            "#492 - Loss nan\n",
            "#493 - Loss nan\n",
            "#494 - Loss nan\n",
            "#495 - Loss nan\n",
            "#496 - Loss nan\n",
            "#497 - Loss nan\n",
            "#498 - Loss nan\n",
            "#499 - Loss nan\n",
            "Saving model. Step:  499\n",
            "Training completed successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXHqM6kHI4hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhLFGQB6I4h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2OTCEtTI4h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVsaPlPyI4h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}