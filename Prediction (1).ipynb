{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib import gridspec\n",
    "import os\n",
    "import time\n",
    "from preprocessing import PreProcessing\n",
    "from model import SiameseNetwork\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import learn\n",
    "import fasttext\n",
    "#import faiss\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_siamese_network/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs = pd.read_csv('./data_repository/questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question1', 'is_duplicate', 'question2'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = ['question1','question2','is_duplicate']\n",
    "question_pairs = question_pairs[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    try:\n",
    "        tk_x = x.lower()\n",
    "\n",
    "        # list of characters which needs to be replaced with space\n",
    "        space_replace_chars = ['?', ':', ',', '\"', '[', ']', '~', '*', ';', '!', '?', '(', ')', '{', '}', '@', '$',\n",
    "                               '#', '.', '-', '/']\n",
    "        tk_x = tk_x.translate({ord(x): ' ' for x in space_replace_chars})\n",
    "\n",
    "        non_space_replace_chars = [\"'\"]\n",
    "        tk_x = tk_x.translate({ord(x): '' for x in non_space_replace_chars})\n",
    "\n",
    "        # remove non-ASCII chars\n",
    "        tk_x = ''.join([c if ord(c) < 128 else '' for c in tk_x])\n",
    "\n",
    "        # replace all consecutive spaces with one space\n",
    "        tk_x = re.sub('\\s+', ' ', tk_x).strip()\n",
    "\n",
    "        # find all consecutive numbers present in the word, first converted numbers to * to prevent conflicts while replacing with numbers\n",
    "        regex = re.compile(r'([\\d])')\n",
    "        tk_x = regex.sub('*', tk_x)\n",
    "        nos = re.findall(r'([\\*]+)', tk_x)\n",
    "        # replace the numbers with the corresponding count like 123 by 3\n",
    "        for no in nos:\n",
    "            tk_x = tk_x.replace(no, \"<NUMBER>\", 1)\n",
    "\n",
    "        return tk_x.strip().lower()\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs['question1'] = question_pairs['question1'].apply(preprocess)\n",
    "question_pairs['question2'] = question_pairs['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs = question_pairs.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_pairs = question_pairs.drop_duplicates('question2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have a question about cosentyx we had a powe...</td>\n",
       "      <td>effects of sacubitril valsartan versus valsart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr requested copies of both paradigm hf and pi...</td>\n",
       "      <td>angiotensin neprilysin inhibition versus enala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dr trimmer initiated and asked for jacinda bla...</td>\n",
       "      <td>cosentyx use in patients with a medical histor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>received &lt;number&gt; &lt;number&gt; information on givi...</td>\n",
       "      <td>package insert myfortic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hcp request for full text of the following two...</td>\n",
       "      <td>medical literature request</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>customer has requested copies of the following...</td>\n",
       "      <td>angiotensin neprilysin inhibition in heart fai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>looking for information on safety of taking co...</td>\n",
       "      <td>cosentyx overdose</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>were there any subsets of patients in the para...</td>\n",
       "      <td>entresto subgroup analysis for the primary end...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>me has a &lt;number&gt; yo caucasian female who repo...</td>\n",
       "      <td>cosentyx relevant medical history of neoplasm ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dr requested a face to face discussion regardi...</td>\n",
       "      <td>request for a medical science liaison msl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  i have a question about cosentyx we had a powe...   \n",
       "1  dr requested copies of both paradigm hf and pi...   \n",
       "2  dr trimmer initiated and asked for jacinda bla...   \n",
       "3  received <number> <number> information on givi...   \n",
       "4  hcp request for full text of the following two...   \n",
       "5  customer has requested copies of the following...   \n",
       "6  looking for information on safety of taking co...   \n",
       "7  were there any subsets of patients in the para...   \n",
       "8  me has a <number> yo caucasian female who repo...   \n",
       "9  dr requested a face to face discussion regardi...   \n",
       "\n",
       "                                           question2 is_duplicate  \n",
       "0  effects of sacubitril valsartan versus valsart...            0  \n",
       "1  angiotensin neprilysin inhibition versus enala...            1  \n",
       "2  cosentyx use in patients with a medical histor...            0  \n",
       "3                            package insert myfortic            0  \n",
       "4                         medical literature request            1  \n",
       "5  angiotensin neprilysin inhibition in heart fai...            1  \n",
       "6                                  cosentyx overdose            1  \n",
       "7  entresto subgroup analysis for the primary end...            1  \n",
       "8  cosentyx relevant medical history of neoplasm ...            1  \n",
       "9          request for a medical science liaison msl            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/swayam/tmp/bert/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "# load vocab_processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(model_path+'vocab')\n",
    "item_db = np.asarray(list(vocab_processor.fit_transform(list(question_pairs['question2']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = fasttext.load_model(model_path+\"ft_skipgram_ws5_dim64.bin\")\n",
    "embeddings_lookup = []\n",
    "for word in list(vocab_processor.vocabulary_._mapping):\n",
    "    try:\n",
    "        embeddings_lookup.append(embeddings_model[str(word)])\n",
    "    except:\n",
    "        pass\n",
    "embeddings_lookup_ = np.asarray(embeddings_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items to be indexed: \t 848\n",
      "Embeddings dimension: \t\t 16\n"
     ]
    }
   ],
   "source": [
    "print('# of items to be indexed: \\t',item_db.shape[0])\n",
    "print('Embeddings dimension: \\t\\t',item_db.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output(feed_data):\n",
    "    checkpoint_file = tf.train.latest_checkpoint(model_path)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            # Load the saved meta graph and restore variables\n",
    "            saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "\n",
    "            # Get the placeholders from the graph by name\n",
    "            anchor_input = graph.get_operation_by_name(\"left_input\").outputs[0]\n",
    "\n",
    "            # Tensors we want to evaluate\n",
    "            predictions = graph.get_operation_by_name(\"output/output_embedding\").outputs[0]\n",
    "\n",
    "            # Collect the predictions here\n",
    "            all_predictions = []\n",
    "\n",
    "            batch_predictions = sess.run(predictions, {anchor_input: feed_data})\n",
    "    return batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs_list = list(question_pairs['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find k nearest neighbour using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Vector representation for each training images and normalize those\n",
    "def generate_db_normed_vectors():\n",
    "    train_vectors = model_output(item_db)\n",
    "    normalized_train_vectors = train_vectors/np.linalg.norm(train_vectors,axis=1).reshape(-1,1)\n",
    "    return normalized_train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find k nearest neighbour using cosine similarity\n",
    "def find_k_nn(normalized_train_vectors,vec,k):\n",
    "    dist_arr = np.matmul(normalized_train_vectors, vec.T)\n",
    "    print(-1*np.sort(-dist_arr.flatten())[:k])\n",
    "    print(max(dist_arr.flatten()))\n",
    "    return np.argsort(-dist_arr.flatten())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File None.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e925c0aed9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormalized_training_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_db_normed_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3918ce5a4597>\u001b[0m in \u001b[0;36mgenerate_db_normed_vectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute Vector representation for each training images and normalize those\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_db_normed_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnormalized_train_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalized_train_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-fc6dc806f8ea>\u001b[0m in \u001b[0;36mmodel_output\u001b[0;34m(feed_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Load the saved meta graph and restore variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}.meta\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/bert/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[1;32m   1452\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                                  **kwargs)[0]\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/bert/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1465\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1466\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1468\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tmp/bert/lib/python3.7/site-packages/tensorflow_core/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File None.meta does not exist."
     ]
    }
   ],
   "source": [
    "normalized_training_vectors = generate_db_normed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB indexing done...\n"
     ]
    }
   ],
   "source": [
    "# Building faiss index\n",
    "db_index = faiss.IndexFlatIP(embedding_dim)  # \n",
    "db_index.add(normalized_training_vectors)  # add vectors to the index\n",
    "print('DB indexing done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_item(query, k=1):\n",
    "    query = [query,'milk']\n",
    "    stime = time.time()\n",
    "    query = [query[0].lower(),query[1].lower()]\n",
    "    input_queries = np.asarray(list(vocab_processor.fit_transform(query)))\n",
    "    search_vector = model_output([input_queries[0]])\n",
    "    normalized_search_vec = search_vector/np.linalg.norm(search_vector)\n",
    "    s_time = time.time()\n",
    "    # candidate_index_i = find_k_nn(normalized_training_vectors, normalized_search_vec, k)\n",
    "    _, candidate_index = db_index.search(normalized_search_vec,k)\n",
    "    candidate_index = candidate_index[0]\n",
    "    print('Total time to find nn: {:0.2f} ms'.format((time.time()-s_time)*1000))    \n",
    "    print('------------------------------------------------------')\n",
    "    print('Query: ',query[0])\n",
    "    print('------------------------------------------------------')\n",
    "    return candidate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_wise_similarity(_left,_right):\n",
    "    similarity_score = []\n",
    "    left_queries = np.asarray(list(vocab_processor.fit_transform(_left)))\n",
    "    right_queries = np.asarray(list(vocab_processor.fit_transform(_right)))\n",
    "    left_vectors = model_output(left_queries)\n",
    "    right_vectors = model_output(right_queries)\n",
    "    \n",
    "    normalized_left_vectors = left_vectors/np.linalg.norm(left_vectors,axis=1).reshape(-1,1)\n",
    "    normalized_right_vectors = right_vectors/np.linalg.norm(right_vectors,axis=1).reshape(-1,1)\n",
    "    for i in range(0,len(normalized_left_vectors)):\n",
    "        similarity_score.append(np.matmul(normalized_left_vectors[i],normalized_right_vectors[i].T))\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_siamese_network/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model_siamese_network/model.ckpt\n",
      "Similarity Score:  [0.85194063]\n"
     ]
    }
   ],
   "source": [
    "question1 = \"How could Quora attract initial users\"\n",
    "question2 = \"When did Quora start and how did it attract users\"\n",
    "similarity_score = get_pair_wise_similarity([question1.lower()],[question2.lower()])\n",
    "print('Similarity Score: ', similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Search [Most similar k questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_siamese_network/model.ckpt\n",
      "Total time to find nn: 5.83 ms\n",
      "------------------------------------------------------\n",
      "Query:  is it healthy to eat egg whites every day\n",
      "------------------------------------------------------\n",
      "is it bad for health to eat eggs every day\n",
      "is it healthy to eat once a day\n",
      "is it unhealthy to eat bananas every day\n",
      "is it healthy to eat bread every day\n",
      "is it healthy to eat fish every day\n",
      "what high protein foods are good for breakfast\n",
      "how do you drink more water every day\n",
      "what will happen if i drink a gallon of milk every day\n",
      "is it healthy to eat one chicken every day\n",
      "is it healthy to eat a whole avocado every day\n"
     ]
    }
   ],
   "source": [
    "query = \"Is it healthy to eat egg whites every day\"\n",
    "candidate_index = get_top_k_item(query.lower(), 10)\n",
    "for index in candidate_index:\n",
    "    print(similar_pairs_list[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                           ---------------------- *** --------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
