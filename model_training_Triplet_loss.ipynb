{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "model_training Triplet loss.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cUWNchP8O9t",
        "colab_type": "code",
        "outputId": "b317ad3a-1d6e-4fa6-90a1-a6b75adc5fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "pip uninstall tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gi1prav-G66",
        "colab_type": "code",
        "outputId": "9b622947-a1c7-47c2-b380-33980232d229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "pip install tensorflow==1.13.2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 63kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.5)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.12.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.29.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em374rCI-e5u",
        "colab_type": "code",
        "outputId": "3ac1d7b5-4937-4e5d-ebba-a74f6477a254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (47.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3016145 sha256=0defeccbda0e3e0040e468af107002583c954cecfd7b02f40f021908ae7b80c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJXpc36R7p3H",
        "colab_type": "code",
        "outputId": "a2517973-c159-48f0-cba4-2b4165c9cd27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import os\n",
        "from matplotlib.image import imread\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.contrib import learn\n",
        "import fasttext\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egGfAsp47p3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_model = fasttext.train_unsupervised(\"text_corpus2.txt\", model='skipgram', lr=0.04, dim=128, epoch=1)\n",
        "\n",
        "embeddings_model.save_model(\"ft_skipgram_ws5_dim28.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwuspQTb7p3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_index = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK4Psi0f7p3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('Triplet_loss_dataset.csv')\n",
        "dataset.dropna(how='any',inplace=True)\n",
        "dataset = shuffle(dataset)\n",
        "dataset = dataset.reset_index(drop = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLFIM0IwApM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removing_bad_characters(test_string):\n",
        "   bad_chars = [';', ':', '!', \"*\",'?','\"',\"'\",'>','<','•','o','≤','â','€','¢'] \n",
        "   # remove bad_chars  \n",
        "   for i in bad_chars : \n",
        "      test_string = test_string.replace(i, '') \n",
        "   return test_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwOWwLp17p3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similar_pairs = pd.DataFrame()\n",
        "similar_pairs['Possible Questions'] = dataset['Possible Questions'].str.lower()\n",
        "similar_pairs['Possible Questions'] = removing_bad_characters(similar_pairs['Possible Questions'])\n",
        "similar_pairs['True_Atom'] = dataset['True_Atom'].str.lower()\n",
        "similar_pairs['True_Atom'] = removing_bad_characters(similar_pairs['True_Atom'])\n",
        "similar_pairs['Atom'] = dataset['Atom'].str.lower()\n",
        "similar_pairs['Atom'] = removing_bad_characters(similar_pairs['Atom'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SpbeBn2zriv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similar_pairs.to_csv('similar_pairs.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK19VdAb7p3h",
        "colab_type": "code",
        "outputId": "17c3d441-1f5b-4510-80e4-db430e93e685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "triplet_list = []\n",
        "for item in similar_pairs.values.tolist():\n",
        "    triplet_list.append(item)\n",
        "print('Triplets created successfully. # of triplets: ',len(triplet_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Triplets created successfully. # of triplets:  6058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwvv3FeGZimD",
        "colab_type": "code",
        "outputId": "b7c16d26-05fe-41dc-934d-73b23b97c03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "triplet_list[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what happens on the coadministraion of probenecid, methotrexate and other medicinal products with furosemide,',\n",
              " 'probenecid, methotrexate and other medicinal products which, like furosemide, are extensively secreted in the renal tubules, can reduce the effect of furosemide.',\n",
              " 'pharmacokinetic studies with cyclosporin have demonstrated that amlodipine does not significantly alter the pharmacokinetics of cyclosporin.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIlVvk6f7p3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplets = triplet_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsD0JBl-7p3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_X = [item[0] for item in triplets]\n",
        "input_Y = [item[1] for item in triplets]\n",
        "input_Z = [item[2] for item in triplets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpZrdaIF7p3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word_count_per_row\n",
        "wc_list_x = list(len(x.split(' ')) for x in input_X)\n",
        "wc_list_y = list(len(x.split(' ')) for x in input_Y)\n",
        "wc_list_z = list(len(x.split(' ')) for x in input_Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XrCv9s7p3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wc_list = []\n",
        "wc_list.extend(wc_list_x)\n",
        "wc_list.extend(wc_list_y)\n",
        "wc_list.extend(wc_list_z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXKUAbRl7p30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_elements = len(input_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWmEHOg97p33",
        "colab_type": "code",
        "outputId": "d7fe4a7e-e16c-4d9e-c46a-7b4c04fcc81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-4c4c9f78fd2d>:1: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pg43jJG7p36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_corpus = []\n",
        "full_corpus.extend(input_X)\n",
        "full_corpus.extend(input_Y)\n",
        "full_corpus.extend(input_Z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JE3eVCh7p39",
        "colab_type": "code",
        "outputId": "a21043f9-d344-4c2d-a5f4-0af6610bf5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "full_data = np.asarray(list(vocab_processor.fit_transform(full_corpus)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wiT0VX97p4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_lookup = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvWYVJgN7p4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for word in list(vocab_processor.vocabulary_._mapping):\n",
        "    try:\n",
        "        embeddings_lookup.append(embeddings_model[str(word)])\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0rNf-h97p4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_lookup = np.asarray(embeddings_lookup)\n",
        "np.save(\"embeddings_lookup.npz\",embeddings_lookup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgJ7Foj77p4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_metadata(filename, labels):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"Index\\tLabel\\n\")\n",
        "        for index, label in enumerate(labels):\n",
        "            f.write(\"{}\\t{}\\n\".format(index, label))\n",
        "    print('Metadata file saved in {}'.format(filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X18kBssz7p4P",
        "colab_type": "code",
        "outputId": "09d1e5a1-7036-46e5-efc8-749d88d7b4eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocab_processor.save('vocab')\n",
        "write_metadata(os.path.join('model_triplet','metadata.tsv'),list(vocab_processor.vocabulary_._mapping))\n",
        "print('Vocab processor executed and saved successfully.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metadata file saved in model_triplet/metadata.tsv\n",
            "Vocab processor executed and saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzZhhiN-7p4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = full_data[0:number_of_elements]\n",
        "Y = full_data[number_of_elements:2*number_of_elements]\n",
        "Z = full_data[2*number_of_elements:3*number_of_elements]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA4dDv8V7p4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_triplets_batch(current_index,n):\n",
        "    last_index = current_index\n",
        "    current_index += n\n",
        "    return X[last_index:current_index,:], Y[last_index:current_index,:],Z[last_index:current_index,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otA-ff-77p4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building model\n",
        "class TripletLoss:\n",
        "\n",
        "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters,output_embedding_size, dropout_keep_prob,embeddings_lookup, l2_reg_lambda=0.0):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.num_filters = num_filters\n",
        "        self.output_embedding_size = output_embedding_size\n",
        "        self.dropout_keep_prob = dropout_keep_prob\n",
        "        self.l2_loss = tf.constant(0.0)\n",
        "        self.embeddings_lookup = embeddings_lookup\n",
        "\n",
        "    def conv_net(self, input_x, reuse=False):\n",
        "        \n",
        "        # Embedding layer\n",
        "        with tf.variable_scope(\"embedding\", reuse=reuse):\n",
        "            self.W = tf.Variable(self.embeddings_lookup, name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "        # Create a convolution + maxpool layer for each filter size\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(self.filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(\n",
        "                    self.embedded_chars_expanded,\n",
        "                    W,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled = tf.nn.max_pool(\n",
        "                    h,\n",
        "                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combine all the pooled features\n",
        "        num_filters_total = self.num_filters * len(self.filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total], name=\"pool_flat\")\n",
        "\n",
        "        # Pre final layer / Embeddings Layer\n",
        "        with tf.variable_scope(\"output\", reuse=reuse):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[num_filters_total, self.output_embedding_size],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b2 = tf.Variable(tf.constant(0.1, shape=[self.output_embedding_size]), name=\"b2\")\n",
        "            self.l2_loss += tf.nn.l2_loss(W)\n",
        "            self.l2_loss += tf.nn.l2_loss(b2)\n",
        "            out_net = tf.nn.xw_plus_b(self.h_pool_flat, W, b2, name=\"scores\")\n",
        "\n",
        "        return out_net\n",
        "\n",
        "    def triplet_loss(self, model_anchor, model_positive, model_negative, margin):\n",
        "        distance1 = tf.sqrt(tf.reduce_sum(tf.pow(model_anchor - model_positive, 2), 1, keepdims=True))\n",
        "        distance2 = tf.sqrt(tf.reduce_sum(tf.pow(model_anchor - model_negative, 2), 1, keepdims=True))\n",
        "        return tf.reduce_mean(tf.maximum(distance1 - distance2 + margin, 0)) + 1e-9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaXNcDEC7p4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Hyperparameters\n",
        "embedding_dim =  128\n",
        "filter_sizes =  \"3,4,5\"\n",
        "num_filters =  128\n",
        "dropout_keep_prob = 0.5\n",
        "l2_reg_lambda = 0.1\n",
        "output_embedding_size = 128\n",
        "batch_size = 100\n",
        "train_iter = 100 #Total training iter\n",
        "step = 20 # Save after ... iteration\n",
        "learning_rate = 0.00005\n",
        "max_document_length = 50 # 'maximum number of word in a sentence'\n",
        "momentum = 0.99\n",
        "#model\n",
        "#data_src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuHIyTdf7p4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUVbdNL67p4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TripletLoss(sequence_length=X.shape[1],\n",
        "                vocab_size=len(vocab_processor.vocabulary_),\n",
        "                embedding_size=embedding_dim,\n",
        "                filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
        "                num_filters=num_filters,\n",
        "                output_embedding_size = output_embedding_size,\n",
        "                dropout_keep_prob = dropout_keep_prob,\n",
        "                embeddings_lookup= embeddings_lookup,\n",
        "                l2_reg_lambda=l2_reg_lambda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJSrT-fX7p4n",
        "colab_type": "code",
        "outputId": "baf3aa36-1b60-4256-ae51-711277ad8c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "placeholder_shape = [None] + [X.shape[1]]\n",
        "print(\"placeholder_shape\", placeholder_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "placeholder_shape [None, 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKpGY6qJ7p4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "next_batch = get_triplets_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyZkhWgh7p4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup Network\n",
        "next_batch = get_triplets_batch\n",
        "anchor_input = tf.placeholder(tf.int32, placeholder_shape, name='anchor_input')\n",
        "positive_input = tf.placeholder(tf.int32, placeholder_shape, name='positive_input')\n",
        "negative_input = tf.placeholder(tf.int32, placeholder_shape, name='negative_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blvrz1eM7p4x",
        "colab_type": "code",
        "outputId": "42c2bf78-89ba-4dc0-8307-27149497ccf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "margin = 3.5\n",
        "anchor_output = model.conv_net(anchor_input, reuse=False)\n",
        "positive_output = model.conv_net(positive_input, reuse=True)\n",
        "negative_output = model.conv_net(negative_input, reuse=True)\n",
        "loss = model.triplet_loss(anchor_output, positive_output, negative_output, margin)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftpc6Tyx7p40",
        "colab_type": "code",
        "outputId": "d6eb66cf-114f-4a8a-a4ef-fae00f9f4b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Setup Optimizer\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "train_step = tf.train.MomentumOptimizer(learning_rate, momentum, \n",
        "                                        use_nesterov=True).minimize(loss,global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk6tUkcL7p44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib.tensorboard.plugins import projector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L-Pf9IlT7p47",
        "colab_type": "code",
        "outputId": "7af841a5-1710-4d05-cf05-89f6bfcbd891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "# Start Training\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Setup Tensorboard\n",
        "    tf.summary.scalar('step', global_step)\n",
        "    tf.summary.scalar('loss', loss)\n",
        "    for var in tf.trainable_variables():\n",
        "        tf.summary.histogram(var.op.name, var)\n",
        "    merged = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter('model_triplet', sess.graph)\n",
        "    # adding embeddings to projector\n",
        "    config = projector.ProjectorConfig()\n",
        "    embed = config.embeddings.add()\n",
        "    embed.tensor_name = \"embedding/W\"\n",
        "    embed.metadata_path = \"metadata.tsv\"\n",
        "    projector.visualize_embeddings(writer, config)\n",
        "    print('Training...')\n",
        "    # Train iter\n",
        "    for i in range(train_iter):\n",
        "        batch_anchor, batch_positive, batch_negative = next_batch(current_index,batch_size)\n",
        "        _, l, summary_str = sess.run([train_step, loss, merged],\n",
        "                                        feed_dict={anchor_input: batch_anchor, positive_input: batch_positive, negative_input: batch_negative})\n",
        "\n",
        "        writer.add_summary(summary_str, i)\n",
        "        print(\"\\r#%d - Loss\" % i, l)\n",
        "\n",
        "        if (i + 1) % step == 0 or l <= 0.1:\n",
        "            print('Saving model. Step: ',i)\n",
        "            saver.save(sess, \"model_triplet/model.ckpt\")\n",
        "        if l<=0.1:\n",
        "          break\n",
        "    saver.save(sess, \"model_triplet/model.ckpt\")\n",
        "print('Training completed successfully.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "#0 - Loss 3.2180228\n",
            "#1 - Loss 3.2025485\n",
            "#2 - Loss 3.1794517\n",
            "#3 - Loss 3.1488092\n",
            "#4 - Loss 3.110696\n",
            "#5 - Loss 3.0651877\n",
            "#6 - Loss 3.0123577\n",
            "#7 - Loss 2.9522796\n",
            "#8 - Loss 2.8850362\n",
            "#9 - Loss 2.8106997\n",
            "#10 - Loss 2.729361\n",
            "#11 - Loss 2.6410973\n",
            "#12 - Loss 2.5459907\n",
            "#13 - Loss 2.4441242\n",
            "#14 - Loss 2.3355913\n",
            "#15 - Loss 2.2204893\n",
            "#16 - Loss 2.0989213\n",
            "#17 - Loss 1.9709837\n",
            "#18 - Loss 1.836784\n",
            "#19 - Loss 1.6964422\n",
            "Saving model. Step:  19\n",
            "#20 - Loss 1.5500811\n",
            "#21 - Loss 1.397844\n",
            "#22 - Loss 1.239871\n",
            "#23 - Loss 1.0780437\n",
            "#24 - Loss 0.9150214\n",
            "#25 - Loss 0.7548722\n",
            "#26 - Loss 0.6015364\n",
            "#27 - Loss 0.47006044\n",
            "#28 - Loss 0.35935864\n",
            "#29 - Loss 0.26355964\n",
            "#30 - Loss 0.18733245\n",
            "#31 - Loss 0.12155407\n",
            "#32 - Loss 0.07108684\n",
            "Saving model. Step:  32\n",
            "Training completed successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FBfbPBc1MgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}